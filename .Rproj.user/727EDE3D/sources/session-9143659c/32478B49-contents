a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2)
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))
# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)
for (i in 1:nreps){
print(i)
# Simulate Y and X
errx = rnorm(n, 0, sigmax)
x = g + errx
y = beta*x + f + rnorm(n, 0, sigma)
# Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
# Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
# Temporary workaround for this step: pretend that m2 is known!
#Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
basis_guess = ortho2(1:(m2*3), 1:(m2*3), s1, s2)
model = lm(y~x + basis_guess)
beta_m2star = summary(model)$coefficients[2,1]
# Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
# that will lead to more efficient estimate
beta_ests = matrix(NA, nrow = B, ncol = M)
for (b in 1:B){
# Sample ytb from the fitted full model above obtained by using m2* df
boot_resids = sample(model$residuals, replace = T)
yboot = beta*x + f + boot_resids
for (d in 1:M){
basis_guess = ortho2(1:d, 1:d, s1, s2)#ortho2(runif(d,0,10), runif(d,0,10), s1, s2)
model_boot_d = lm(yboot ~ x + basis_guess)
beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
}
}
# a) calculate average of bootstrapped estimates for each d
avg_boot[i,] = colMeans(beta_ests)
# b) the unconditional squared bias (USB)
usbs[i,] = (avg_boot[i,] - beta_m2star)^2
uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}
boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
plot(1:M, colMeans(usbs), type = 'l', main = 'Unconditional Squared Bias', xlab = 'd', ylim = c(0,0.02))
plot(1:M, colMeans(uvs), type = 'l', main = 'Unconditional Variance', xlab = 'd')
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', main = 'Unconditional MSE', xlab = 'd')
plot(1:M, colMeans(usbs), type = 'l', main = 'Unconditional Squared Bias', xlab = 'd', ylim = c(0,0.02))
lines(1:M, colMeans(uvs), type = 'l', main = 'Unconditional Variance', xlab = 'd')
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', main = 'Unconditional MSE', xlab = 'd')
plot(1:M, colMeans(usbs), type = 'l', main = 'Unconditional Squared Bias', xlab = 'd', ylim = c(0,0.025))
lines(1:M, colMeans(uvs), type = 'l', main = 'Unconditional Variance', xlab = 'd')
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', main = 'Unconditional MSE', xlab = 'd')
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Perfect Basis Selection')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'))
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Perfect Basis Selection')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Perfect Basis Selection: g smoother than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))
set.seed(99)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 4
m2 = 10
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15
a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2)
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))
# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)
for (i in 1:nreps){
print(i)
# Simulate Y and X
errx = rnorm(n, 0, sigmax)
x = g + errx
y = beta*x + f + rnorm(n, 0, sigma)
# Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
# Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
# Temporary workaround for this step: pretend that m2 is known!
#Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
basis_guess = ortho2(1:(m2*2), 1:(m2*2), s1, s2)
model = lm(y~x + basis_guess)
beta_m2star = summary(model)$coefficients[2,1]
# Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
# that will lead to more efficient estimate
beta_ests = matrix(NA, nrow = B, ncol = M)
for (b in 1:B){
# Sample ytb from the fitted full model above obtained by using m2* df
boot_resids = sample(model$residuals, replace = T)
yboot = beta*x + f + boot_resids
for (d in 1:M){
basis_guess = ortho2(1:d, 1:d, s1, s2)#ortho2(runif(d,0,10), runif(d,0,10), s1, s2)
model_boot_d = lm(yboot ~ x + basis_guess)
beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
}
}
# a) calculate average of bootstrapped estimates for each d
avg_boot[i,] = colMeans(beta_ests)
# b) the unconditional squared bias (USB)
usbs[i,] = (avg_boot[i,] - beta_m2star)^2
uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}
boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Perfect Basis Selection: g rougher than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))
round(ortho2(runif(d,0,10), runif(d,0,10), s1, s2)%*%t(ortho2(runif(d,0,10), runif(d,0,10), s1, s2)),digits = 3)
round(t(ortho2(runif(d,0,10), runif(d,0,10), s1, s2))%*%ortho2(runif(d,0,10), runif(d,0,10), s1, s2),digits = 3)
round(t(ortho2(runif(d,0,10), runif(d,0,10), s1, s2))%*%ortho2(runif(d,0,10), runif(d,0,10), s1, s2),digits = 3)/10
length(s1)
round(t(ortho2(runif(d,0,10), runif(d,0,10), s1, s2))%*%ortho2(runif(d,0,10), runif(d,0,10), s1, s2),digits = 3)/100
round(t(ortho2(runif(5,0,10), runif(5,0,10), s1, s2))%*%ortho2(runif(d,0,10), runif(d,0,10), s1, s2),digits = 3)/100
round(t(ortho2(1:5, 1:5, s1, s2))%*%ortho2(runif(d,0,10), runif(d,0,10), s1, s2),digits = 3)/100
round(t(ortho2(1:5, 1:5, s1, s2))%*%ortho2(1:5, 1:5, s1, s2),digits = 3)/100
round(t(ortho2(runif(5,0,10), runif(5,0,10), s1, s2))%*%ortho2(runif(5,0,10), runif(5,0,10), s1, s2),digits = 3)/100
test = runif(5,0,10)
round(t(ortho2(test, test, s1, s2))%*%ortho2(test, test, s1, s2),digits = 3)/100
test2 = runif(5,0,10)
round(t(ortho2(test, test2, s1, s2))%*%ortho2(test, test2, s1, s2),digits = 3)/100
test = runif(5)
test2 = runif(5)
round(t(ortho2(test, test2, s1, s2))%*%ortho2(test, test2, s1, s2),digits = 3)/100
test2 = runif(5,0,5)
test = runif(5,0,5)
round(t(ortho2(test, test2, s1, s2))%*%ortho2(test, test2, s1, s2),digits = 3)/100
test2 = runif(5,0,3)
test = runif(5,0,3)
round(t(ortho2(test, test2, s1, s2))%*%ortho2(test, test2, s1, s2),digits = 3)/100
set.seed(99)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15
a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2)
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))
# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)
for (i in 1:nreps){
print(i)
# Simulate Y and X
errx = rnorm(n, 0, sigmax)
x = g + errx
y = beta*x + f + rnorm(n, 0, sigma)
# Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
# Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
# Temporary workaround for this step: pretend that m2 is known!
#Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
basis_guess = ortho2(1:(m2*3), 1:(m2*3), s1, s2)
model = lm(y~x + basis_guess)
beta_m2star = summary(model)$coefficients[2,1]
# Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
# that will lead to more efficient estimate
beta_ests = matrix(NA, nrow = B, ncol = M)
for (b in 1:B){
# Sample ytb from the fitted full model above obtained by using m2* df
boot_resids = sample(model$residuals, replace = T)
yboot = beta*x + f + boot_resids
for (d in 1:M){
basis_guess = ortho2(runif(d,0,d), runif(d,0,d), s1, s2) # should I use a consistent wrong basis?
model_boot_d = lm(yboot ~ x + basis_guess)
beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
}
}
# a) calculate average of bootstrapped estimates for each d
avg_boot[i,] = colMeans(beta_ests)
# b) the unconditional squared bias (USB)
usbs[i,] = (avg_boot[i,] - beta_m2star)^2
uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}
boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Wrong Basis Selection for f, g smoother than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))
set.seed(99)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15
a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2)
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))
# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)
for (i in 1:nreps){
print(i)
# Simulate Y and X
errx = rnorm(n, 0, sigmax)
x = g + errx
y = beta*x + f + rnorm(n, 0, sigma)
# Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
# Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
# Temporary workaround for this step: pretend that m2 is known!
#Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
basis_guess = ortho2(runif(m2*3,0,m2*3), runif(m2*3,0,m2*3), s1, s2)
model = lm(y~x + basis_guess)
beta_m2star = summary(model)$coefficients[2,1]
# Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
# that will lead to more efficient estimate
beta_ests = matrix(NA, nrow = B, ncol = M)
for (b in 1:B){
# Sample ytb from the fitted full model above obtained by using m2* df
boot_resids = sample(model$residuals, replace = T)
yboot = beta*x + f + boot_resids
for (d in 1:M){
basis_guess = ortho2(runif(d,0,d), runif(d,0,d), s1, s2) # should I use a consistent wrong basis?
model_boot_d = lm(yboot ~ x + basis_guess)
beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
}
}
# a) calculate average of bootstrapped estimates for each d
avg_boot[i,] = colMeans(beta_ests)
# b) the unconditional squared bias (USB)
usbs[i,] = (avg_boot[i,] - beta_m2star)^2
uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}
boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Wrong Basis Selection for f, g smoother than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))
wrongbasisvec = list(runif(M,0,M), runif(M,0,M))
wrongbasisvec
wrongbasisvec[[1]][1:d]
set.seed(99)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15
wrongbasisvec = list(runif(M,0,M), runif(M,0,M))
a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2)
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))
# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)
for (i in 1:nreps){
print(i)
# Simulate Y and X
errx = rnorm(n, 0, sigmax)
x = g + errx
y = beta*x + f + rnorm(n, 0, sigma)
# Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
# Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
# Temporary workaround for this step: pretend that m2 is known!
#Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
basis_guess = ortho2(1:(m2*3), 1:(m2*3), s1, s2)
model = lm(y~x + basis_guess)
beta_m2star = summary(model)$coefficients[2,1]
# Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
# that will lead to more efficient estimate
beta_ests = matrix(NA, nrow = B, ncol = M)
for (b in 1:B){
# Sample ytb from the fitted full model above obtained by using m2* df
boot_resids = sample(model$residuals, replace = T)
yboot = beta*x + f + boot_resids
for (d in 1:M){
basis_guess = ortho2(wrongbasisvec[[1]][1:d], wrongbasisvec[[2]][1:d],
s1, s2)
#ortho2(runif(d,0,d), runif(d,0,d), s1, s2) # should I use a consistent wrong basis?
model_boot_d = lm(yboot ~ x + basis_guess)
beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
}
}
# a) calculate average of bootstrapped estimates for each d
avg_boot[i,] = colMeans(beta_ests)
# b) the unconditional squared bias (USB)
usbs[i,] = (avg_boot[i,] - beta_m2star)^2
uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}
boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Wrong Basis Selection for f, g smoother than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))
set.seed(100)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15
wrongbasisvec = list(runif(M,0,M), runif(M,0,M))
a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2)
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))
# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)
for (i in 1:nreps){
print(i)
# Simulate Y and X
errx = rnorm(n, 0, sigmax)
x = g + errx
y = beta*x + f + rnorm(n, 0, sigma)
# Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
# Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
# Temporary workaround for this step: pretend that m2 is known!
#Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
basis_guess = ortho2(1:(m2*3), 1:(m2*3), s1, s2)
model = lm(y~x + basis_guess)
beta_m2star = summary(model)$coefficients[2,1]
# Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
# that will lead to more efficient estimate
beta_ests = matrix(NA, nrow = B, ncol = M)
for (b in 1:B){
# Sample ytb from the fitted full model above obtained by using m2* df
boot_resids = sample(model$residuals, replace = T)
yboot = beta*x + f + boot_resids
for (d in 1:M){
basis_guess = ortho2(wrongbasisvec[[1]][1:d], wrongbasisvec[[2]][1:d],
s1, s2)
#ortho2(runif(d,0,d), runif(d,0,d), s1, s2) # should I use a consistent wrong basis?
model_boot_d = lm(yboot ~ x + basis_guess)
beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
}
}
# a) calculate average of bootstrapped estimates for each d
avg_boot[i,] = colMeans(beta_ests)
# b) the unconditional squared bias (USB)
usbs[i,] = (avg_boot[i,] - beta_m2star)^2
uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}
boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Wrong Basis Selection for f, g smoother than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))
set.seed(99)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 4
m2 = 10
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15
a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2)
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))
# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)
for (i in 1:nreps){
print(i)
# Simulate Y and X
errx = rnorm(n, 0, sigmax)
x = g + errx
y = beta*x + f + rnorm(n, 0, sigma)
# Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
# Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
# Temporary workaround for this step: pretend that m2 is known!
#Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
basis_guess = ortho2(1:(m2*3), 1:(m2*3), s1, s2)
model = lm(y~x + basis_guess)
beta_m2star = summary(model)$coefficients[2,1]
# Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
# that will lead to more efficient estimate
beta_ests = matrix(NA, nrow = B, ncol = M)
for (b in 1:B){
# Sample ytb from the fitted full model above obtained by using m2* df
boot_resids = sample(model$residuals, replace = T)
yboot = beta*x + f + boot_resids
for (d in 1:M){
basis_guess = ortho2(runif(d,0,d), runif(d,0,d), s1, s2) # should I use a consistent wrong basis?
model_boot_d = lm(yboot ~ x + basis_guess)
beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
}
}
# a) calculate average of bootstrapped estimates for each d
avg_boot[i,] = colMeans(beta_ests)
# b) the unconditional squared bias (USB)
usbs[i,] = (avg_boot[i,] - beta_m2star)^2
uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}
boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB
plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
main = 'Wrong Basis Selection for f, g rougher than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))
