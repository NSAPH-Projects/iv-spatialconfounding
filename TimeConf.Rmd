---
title: "Semiparametric Time Series Confounding"
author: "Sophie Woodward"
date: '2023-02-21'
output: pdf_document
---

## Replication of 
```{r}
# 3d plotting
install.packages("scatterplot3d")
library("scatterplot3d") 

# Creates an orthogonal function basis from points in R2
# change u,v to change basis
# u, v can be anything as long as same length - some choices result in better orthogonality than others
ortho2 = function(u, v, s1, s2){ # u and v change basis 
  mat = matrix(NA, nrow = length(s1), ncol = length(u))
  for (i in 1:nrow(mat)){
    for (j in 1:ncol(mat)){
      mat[i,j] = cos(2*pi*(u[j]*s1[i] + v[j]*s2[i])) + sin(2*pi*(u[j]*s1[i] + v[j]*s2[i]))
    }
  }
  return(mat)
}

ortho2 = function(u, v, s1, s2){ # u and v change basis 
  cos_mat = outer(s1, u, FUN = "*") + outer(s2, v, FUN = "*")
  sin_mat = outer(s1, u, FUN = "*") + outer(s2, v, FUN = "*")
  cos_mat = cospi(2*cos_mat)
  sin_mat = sinpi(2*sin_mat)
  mat = cos_mat + sin_mat
  return(mat)
}

```

# Scenario A: g(t) is more smooth than f(t)
```{r}
t = seq(0,1,by = 0.001)

# Orthonormal basis
ortho = function(n, t){ # n in natural nums
  mat = matrix(NA, nrow = length(t), ncol = length(n))
  for (i in 1:length(n)){
    for (j in 1:length(t)){
      mat[j,i] = sqrt(2)*sin(2*pi*n[i]*t[j])
    }
  }
  return(mat)
}

m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
a = rnorm(m1)
b = rnorm(m2)
#a0 = 2
#b0 = 2
#f = rowSums(sapply(1:m1, ortho) %*% diag(a)) #+ a0
f = rowSums(ortho(n = 1:m1, t = t) %*% diag(a))
g = rowSums(ortho(n = 1:m2, t = t) %*% diag(b))

#g = rowSums(sapply(1:m2, ortho) %*% diag(b)) #+ b0
# Check
plot(t, f, type = 'l')
lines(t, g, type = 'l', col = 'red')
legend('topleft', legend = c('f', 'g'), col = c('black', 'red'), lty = 1:1)

# Model Y and X
beta = -1
errx = rnorm(length(t), 0, sigmax)
x = g + errx
y = beta*x + f + rnorm(length(t), 0, sigma)
plot(x,y)

# Estimate bias of bq
nreps = 10000
betas = rep(NA, nreps)
for (i in 1:nreps){
  y = beta*x + f + rnorm(length(t), 0, sigma)
  model = lm(y~x + ortho(n = 1:m2, t = t)) # what if this is fit on a different basis
  betas[i] = summary(model)$coefficients[2,1]
}

mean(betas) - beta

(t(errx) %*% ortho((m2+1):m1, t) %*% a[(m2+1):m1])/
  (t(errx) %*% (diag(1, nrow = length(t)) - ortho(1:m2, t)%*%t(ortho(1:m2, t))/1001) %*% errx)

var(betas)

sigma^2/(t(errx) %*% (diag(1, nrow = length(t)) - ortho(1:m2, t)%*%t(ortho(1:m2, t))/1001) %*% errx)


# Check orthogonality: should be identity
#round(t(ortho(1:m1,t))%*%ortho(1:m1, t)/1001, digits = 10)
```

## Spatial Confounding
```{r}
t1 = seq(0,1,by = 0.001)
t2 = seq(0,1,by = 0.001)

# Orthonormal basis
ortho2 = function(n, t1, t2){ # n in natural nums
  mat = array(NA, dim=c(length(t1), length(t2), length(n)))
  for (i in 1:length(n)){
    for (j in 1:length(t1)){
      for (k in 1:length(t2)){
          mat[j,k,i] = sqrt(2)*sin(2*pi*n[i]*(t1[j] + t2[k]))
      }
    }
  }
  return(mat)
}

m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
a = rnorm(m1)
b = rnorm(m2)
#a0 = 2
#b0 = 2
#f = rowSums(sapply(1:m1, ortho2) %*% diag(a)) #+ a0
orthobasis_m1 = ortho2(n = 1:m1, t1, t2)
orthobasis_m2 = ortho2(n = 1:m2, t1, t2)

f = matrix(0, nrow = length(t1), ncol = length(t2))
for (k in 1:m1){
  f = f + a[k]*orthobasis_m1[,,k]
}
g = matrix(0, nrow = length(t1), ncol = length(t2))
for (k in 1:m2){
  g = g + b[k]*orthobasis_m2[,,k]
}


#g = rowSums(sapply(1:m2, ortho2) %*% diag(b)) #+ b0
# Check
plot(t1, g[2,], type = 'l')
lines(t, g, type = 'l', col = 'red')
legend('topleft', legend = c('f', 'g'), col = c('black', 'red'), lty = 1:1)

# Model Y and X
# Here just sample a few points from the t1 t2 grid - too computationally expensive
beta = -2
n = 500
s1 = sample(1:length(t1), n)# sample n points uniformly from t1 x t2 grid
s2 = sample(1:length(t2), n)
errx = rnorm(n, 0, sigmax)
x = errx#g + errx # evaluate g at s only
ybase = rep(NA, n)
for (i in 1:n){
  x[i] = x[i] + g[s1[i],s2[i]] 
  ybase[i] = beta*x[i] + f[s1[i],s2[i]]
}
plot(x,ybase) # 2d plot would be best here

# change later
basism2 = matrix(NA, nrow = n, ncol = m2)
for (i in 1:n){
  for (j in 1:m2){
    basism2[i,j] = orthobasis_m2[s1[i],s2[i],j]
  }
}

basism1 = matrix(NA, nrow = n, ncol = m1)
for (i in 1:n){
  for (j in 1:m1){
    basism1[i,j] = orthobasis_m1[s1[i],s2[i],j]
  }
}

# Estimate bias of bq
nreps = 10000
betas = rep(NA, nreps)
for (i in 1:nreps){
  y = ybase + rnorm(n, 0, sigma)
  model = lm(y~x + basism2)
  # above: instead of ortho2, regressing on m2 basis functions evaluated at locations s??
  betas[i] = summary(model)$coefficients[2,1]
}

mean(betas) - beta

#(t(errx) %*% ortho((m2+1):m1, t) %*% a[(m2+1):m1])/
#  (t(errx) %*% (diag(1, nrow = length(t)) - ortho(1:m2, t)%*%t(ortho(1:m2, t))/1001) %*% errx)

(t(errx) 
  %*% basism1[,(m2+1):m1]
  %*% a[(m2+1):m1])/
  (t(errx) %*% (diag(1, nrow = n) - basism1[,1:m2]%*%t(basism1[,1:m2])/1000) %*% errx)

var(betas)

sigma^2/(t(errx) %*% (diag(1, nrow = n) - basism1[,1:m2]%*%t(basism1[,1:m2])/1000) %*% errx)

```

# Scenario A: g(s) is more smooth than f(s)
```{r}
n = 1000
s1 = runif(n)
s2 = runif(n)
m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(runif(m1, 0, m1), runif(m1, 0, m1), s1, s2) 
orthobasis_m2 = ortho2(runif(m2, 0, m2), runif(m2, 0, m2), s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))

# Check
scatterplot3d(s1, s2, g, angle = 55)
scatterplot3d(s1, s2, g, angle = 10)
scatterplot3d(s1, s2, f, angle = 0)
scatterplot3d(s1, s2, f, angle = 20)

# Model Y and X
beta = 0
errx = rnorm(n, 0, sigmax)
x = g + errx

nreps = 10000
betas = rep(NA, nreps)
for (i in 1:nreps){
  y = beta*x + f + rnorm(n, 0, sigma)
  model = lm(y~x + orthobasis_m2) # what if this is fit on a different basis
  betas[i] = summary(model)$coefficients[2,1]
}

mean(betas) - beta

(t(errx) 
  %*% orthobasis_m1[,(m2+1):m1]
  %*% a[(m2+1):m1])/
  (t(errx) %*% (diag(1, nrow = n) - orthobasis_m1[,1:m2]%*%t(orthobasis_m1[,1:m2])/n) %*% errx)

var(betas)

sigma^2/(t(errx) %*% (diag(1, nrow = n) - orthobasis_m1[,1:m2]%*%t(orthobasis_m1[,1:m2])/n) %*% errx)

```

# Scenario B: g(s) rougher than f(s)
```{r}
n = 1000
s1 = runif(n)
s2 = runif(n)
m1 = 4
m2 = 10
sigma = 0.17
sigmax = 3
a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(runif(m1, 0, m1), runif(m1, 0, m1), s1, s2) 
orthobasis_m2 = ortho2(runif(m2, 0, m2), runif(m2, 0, m2), s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))

# Check
scatterplot3d(s1, s2, g, angle = 55)
scatterplot3d(s1, s2, g, angle = 10)
scatterplot3d(s1, s2, f, angle = 0)
scatterplot3d(s1, s2, f, angle = 20)

# Model Y and X
beta = 0
errx = rnorm(n, 0, sigmax)
x = g + errx

nreps = 10000
betas = rep(NA, nreps)
for (i in 1:nreps){
  y = beta*x + f + rnorm(n, 0, sigma)
  model = lm(y~x + orthobasis_m2) # what if this is fit on a different basis
  betas[i] = summary(model)$coefficients[2,1]
}

mean(betas) - beta # should equal 0

var(betas)

sigma^2/(t(errx) %*% (diag(1, nrow = n) - orthobasis_m2%*%t(orthobasis_m2)/n) %*% errx)

```

## Bootstrap analysis: g smoother than f
```{r}
set.seed(99)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15

a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2) 
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))

# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)

for (i in 1:nreps){
  print(i)
  # Simulate Y and X
  errx = rnorm(n, 0, sigmax)
  x = g + errx
  y = beta*x + f + rnorm(n, 0, sigma)
  
  # Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
  # Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
  # Temporary workaround for this step: pretend that m2 is known!
  
  #Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
  basis_guess = ortho2(1:(m2*3), 1:(m2*3), s1, s2)
  model = lm(y~x + basis_guess)
  beta_m2star = summary(model)$coefficients[2,1]
  
  # Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
  # that will lead to more efficient estimate
  beta_ests = matrix(NA, nrow = B, ncol = M)
  for (b in 1:B){
    # Sample ytb from the fitted full model above obtained by using m2* df
    boot_resids = sample(model$residuals, replace = T)
    yboot = beta*x + f + boot_resids
    for (d in 1:M){
      basis_guess = ortho2(1:d, 1:d, s1, s2)#ortho2(runif(d,0,10), runif(d,0,10), s1, s2)
      model_boot_d = lm(yboot ~ x + basis_guess)
      beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
    }
  }
  # a) calculate average of bootstrapped estimates for each d
  avg_boot[i,] = colMeans(beta_ests)
  # b) the unconditional squared bias (USB)
  usbs[i,] = (avg_boot[i,] - beta_m2star)^2
  uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}

boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB

plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
     main = 'Perfect Basis Selection: g smoother than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))

```

# Bootstrap analysis: g rougher than f
```{r}
set.seed(99)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 4
m2 = 10
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15

a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2) 
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))

# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)

for (i in 1:nreps){
  print(i)
  # Simulate Y and X
  errx = rnorm(n, 0, sigmax)
  x = g + errx
  y = beta*x + f + rnorm(n, 0, sigma)
  
  # Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
  # Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
  # Temporary workaround for this step: pretend that m2 is known!
  
  #Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
  basis_guess = ortho2(1:(m2*2), 1:(m2*2), s1, s2)
  model = lm(y~x + basis_guess)
  beta_m2star = summary(model)$coefficients[2,1]
  
  # Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
  # that will lead to more efficient estimate
  beta_ests = matrix(NA, nrow = B, ncol = M)
  for (b in 1:B){
    # Sample ytb from the fitted full model above obtained by using m2* df
    boot_resids = sample(model$residuals, replace = T)
    yboot = beta*x + f + boot_resids
    for (d in 1:M){
      basis_guess = ortho2(1:d, 1:d, s1, s2)#ortho2(runif(d,0,10), runif(d,0,10), s1, s2)
      model_boot_d = lm(yboot ~ x + basis_guess)
      beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
    }
  }
  # a) calculate average of bootstrapped estimates for each d
  avg_boot[i,] = colMeans(beta_ests)
  # b) the unconditional squared bias (USB)
  usbs[i,] = (avg_boot[i,] - beta_m2star)^2
  uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}

boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB

plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
     main = 'Perfect Basis Selection: g rougher than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))

```

# Bootstrap analysis: g smoother than f, incorrect f basis 
```{r}
set.seed(100)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 10
m2 = 4
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15

wrongbasisvec = list(runif(M,0,M), runif(M,0,M))

a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2) 
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))

# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)

for (i in 1:nreps){
  print(i)
  # Simulate Y and X
  errx = rnorm(n, 0, sigmax)
  x = g + errx
  y = beta*x + f + rnorm(n, 0, sigma)
  
  # Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
  # Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
  # Temporary workaround for this step: pretend that m2 is known!
  
  #Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
  basis_guess = ortho2(1:(m2*3), 1:(m2*3), s1, s2)
  model = lm(y~x + basis_guess)
  beta_m2star = summary(model)$coefficients[2,1]
  
  # Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
  # that will lead to more efficient estimate
  beta_ests = matrix(NA, nrow = B, ncol = M)
  for (b in 1:B){
    # Sample ytb from the fitted full model above obtained by using m2* df
    boot_resids = sample(model$residuals, replace = T)
    yboot = beta*x + f + boot_resids
    for (d in 1:M){
      basis_guess = ortho2(wrongbasisvec[[1]][1:d], wrongbasisvec[[2]][1:d], 
                           s1, s2)
      #ortho2(runif(d,0,d), runif(d,0,d), s1, s2) # should I use a consistent wrong basis?
      model_boot_d = lm(yboot ~ x + basis_guess)
      beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
    }
  }
  # a) calculate average of bootstrapped estimates for each d
  avg_boot[i,] = colMeans(beta_ests)
  # b) the unconditional squared bias (USB)
  usbs[i,] = (avg_boot[i,] - beta_m2star)^2
  uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}

boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB

plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
     main = 'Wrong Basis Selection for f, g smoother than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))

```

# Boostrap analysis: g rougher than f, incorrect basis for f
```{r}
set.seed(99)
n = 100
s1 = runif(n)
s2 = runif(n)
beta = 0
m1 = 4
m2 = 10
sigma = 0.17
sigmax = 3
nreps = 50
B = 100
M = 15

a = rnorm(m1)
b = rnorm(m2)
a0 = 1
b0 = -1
orthobasis_m1 = ortho2(1:m1, 1:m1, s1, s2) 
orthobasis_m2 = ortho2(1:m2, 1:m2, s1, s2)
f = a0 + rowSums(orthobasis_m1 %*% diag(a))
g = b0 + rowSums(orthobasis_m2 %*% diag(b))

# Unconditional Squared Bias
usbs = matrix(NA, nrow = nreps, ncol = M)
# Unconditional variances
uvs = matrix(NA, nrow = nreps, ncol = M)
avg_boot = matrix(NA, nrow = nreps, ncol = M)

for (i in 1:nreps){
  print(i)
  # Simulate Y and X
  errx = rnorm(n, 0, sigmax)
  x = g + errx
  y = beta*x + f + rnorm(n, 0, sigma)
  
  # Estimate m2 so that g(t) is well modeled in the spline representation to adequately predict xt .
  # Generalized cross-validation (GCV) methods (Hastie and Tibshirani 1990; Hastie, Tibshirani, and Buja 1993) can be used to estimate d.
  # Temporary workaround for this step: pretend that m2 is known!
  
  #Fit the model y = beta x + f(t) + ep by representing f(t) with m2* = 3*m2 basis functions
  basis_guess = ortho2(1:(m2*3), 1:(m2*3), s1, s2)
  model = lm(y~x + basis_guess)
  beta_m2star = summary(model)$coefficients[2,1]
  
  # Implement the bootstrap analysis for identifying a number of degrees of freedom smaller than m2*
  # that will lead to more efficient estimate
  beta_ests = matrix(NA, nrow = B, ncol = M)
  for (b in 1:B){
    # Sample ytb from the fitted full model above obtained by using m2* df
    boot_resids = sample(model$residuals, replace = T)
    yboot = beta*x + f + boot_resids
    for (d in 1:M){
      basis_guess = ortho2(runif(d,0,d), runif(d,0,d), s1, s2) # should I use a consistent wrong basis?
      model_boot_d = lm(yboot ~ x + basis_guess)
      beta_ests[b,d] = summary(model_boot_d)$coefficients[2,1]
    }
  }
  # a) calculate average of bootstrapped estimates for each d
  avg_boot[i,] = colMeans(beta_ests)
  # b) the unconditional squared bias (USB)
  usbs[i,] = (avg_boot[i,] - beta_m2star)^2
  uvs[i,] = 1/(B-1)*colSums((beta_ests - avg_boot[i,])^2)
}

boxplot(avg_boot, main = 'Average of Bootstrap Estimates', xlab = 'd', ylab = '1/B sum_b betahat_d^{b,i}')
# USB

plot(1:M, colMeans(usbs), type = 'l', xlab = 'd', ylim = c(0,0.025), col = 'red',
     main = 'Wrong Basis Selection for f, g rougher than f')
# UV
lines(1:M, colMeans(uvs), type = 'l',  col = 'blue')
# MSE
lines(1:M, colMeans(usbs) + colMeans(uvs), type = 'l', col = 'black')
abline(v = m1, lty = 2)
legend('topright', legend = c('Unconditional Squared Bias', 'Unconditional Variance', 'Unconditional MSE'), col = c('red', 'blue', 'black'), lty = c(1,1,1))

```






